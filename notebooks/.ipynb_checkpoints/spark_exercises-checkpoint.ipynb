{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i0u19a - Data Processing - KU Leuven\n",
    "\n",
    "# Python Spark exercises\n",
    "\n",
    "###### _Thomas Moerman, Jan Aerts_\n",
    "\n",
    "![license](https://licensebuttons.net/l/by/3.0/88x31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello and welcome to the tutorial on data processing with Spark!\n",
    "\n",
    "We'll be using Jupyter notebook (you're looking at it) as a tool to walk you through a few examples. At the VDA-LAB, we like notebooks as a teaching tool because they allow you to experiment with code and data as you work your way through the document.\n",
    "\n",
    "A few guidelines on the notebook itself:\n",
    "* A notebook consists of *cells*, which are snippets of either text (markdown) or code (Python in this case).\n",
    "* Cells can be executed by clicking the `[>]` \"play\" button, or by hitting shift-enter on the keyboard.\n",
    "* You can navigate between cells either by clicking or by using the arrow buttons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Sparky\n",
    "\n",
    "The `SparkContext` object is the main entrypoint for every Spark program. \n",
    "\n",
    "We instantiate a `SparkContext` object and store it in variable 'sc'. We only need to do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "# prevents ValueError from occurring when reloading this cell\n",
    "def init_sc(prev_sc):  \n",
    "    try:        \n",
    "        return pyspark.SparkContext('local[*]')\n",
    "    except ValueError:\n",
    "        return prev_sc        \n",
    "\n",
    "sc = init_sc(locals().get('sc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the `SparkContext` in order to create RDDs: **Resilient Distributed Datasets.**\n",
    "\n",
    "RDDs are the main building block in Spark programs. They represent distributed computations, partitioned over a cluster of machines. But we don't worry too much about that, let's for now do something trivial to verify whether it works.\n",
    "\n",
    "We turn a range of 1000 integers in an RDD with the `parallelize(data)` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Spark doesn't compute any results right away. Think of it as storing the \"recipe\" for the computation in a variable.\n",
    "\n",
    "To actually launch a computation, we can perform a `collect` or a `take` operation on the rdd. Let's try taking the top 3 elements from the rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy!\n",
    "\n",
    "Now, why is this approach a convenient? Think about it for few moments.\n",
    "\n",
    "Imagine you wrote a complicated computation on a large dataset. This could take a while to compute. When writing and testing a computation, we can go along while only inspecting a small subset of the result by performing for example a `take(5)` operation. This way we can avoid always having to wait for the entire computation to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping\n",
    "\n",
    "Our boss decided that our sequence should not start at 0 but at 10. Of course we could re-initialize the rdd with the correct range, but let's for the sake of the exercise transform the `rdd` data set.\n",
    "\n",
    "We need a function that allows us to increment each number in the sequence with 10.\n",
    "\n",
    "That function is the **`map` function**, which **takes a function and applies it to every single element of a collection/array**. For example: 'multiply all elements by 2'. This means that the length of the output array will be the same as the length of the input array.\n",
    "\n",
    "Let's first define that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inc10(i):\n",
    "    return i + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the `inc10` function as argument to the `map` method on our rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_inc10 = rdd.map(inc10)\n",
    "\n",
    "rdd_inc10.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. \n",
    "\n",
    "Now, instead of passing a named function, we can also pass an anonymous function function to the `map` method. Python uses lambda expressions for inline anonymous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_inc10_anon = rdd.map(lambda i: i + 10)\n",
    "\n",
    "rdd_inc10_anon.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that works as well. Often it is more convenient to use anonymous functions, but it all depends on the preference of the programmer of course, there's no right or wrong in this matter.\n",
    "\n",
    "### Filtering\n",
    "\n",
    "Let's now make our range of numbers a bit more interesting. We are now only interested in even numbers. In order to express this, we need to `filter` our rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_even(x):\n",
    "    return x % 2 == 0 # complete this\n",
    "\n",
    "rdd_even = rdd_inc10.filter(is_even) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `filter` method takes a function or lambda expression that takes an integer `i` and produces a `boolean` value that decides whether or not to keep the value `i` in the resulting rdd.\n",
    "\n",
    "Let's test by taking a random sample of 3 items from the rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "even_3 = rdd_even.takeSample(False, 3)\n",
    "\n",
    "for i in even_3:\n",
    "    assert is_even(i)\n",
    "    \n",
    "even_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only even numbers, right? Good.\n",
    "\n",
    "### Reducing\n",
    "\n",
    "Let's now perform an aggregation on the data. The workhorse function for simple aggregations is the `reduce(f)` function. Look up the `reduce` function in the [pyspark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) page. It says:\n",
    "\n",
    "**`reduce(f)`**\n",
    "\n",
    "> Reduces the elements of this RDD using the specified _commutative and associative binary operator_.\n",
    "\n",
    "A binary operator is a function that takes 2 inputs and produces one output. IMPORTANT: inputs and outputs have the **same type**. Can you come up with some examples of binary operators?\n",
    "\n",
    "Straightforward examples are mathematical operators like addition (`+`) and multiplication (`*`). Note that these operators are not restricted to numbers, we can also define commutative and associative binary operators on other data types like hashmaps, as we'll see in a later example.\n",
    "\n",
    "Let's create a simple RDD and try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_tiny = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "rdd_tiny.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction goes as follows: \n",
    "\n",
    "* First, we take the first and second element and apply the function `f` to these inputs (remember, `f` is the mathematical addition):\n",
    "\n",
    "    `f(1, 2) => 3`\n",
    "    \n",
    "    \n",
    "* Next, we take the result `3` and use it in the next calculation:\n",
    "\n",
    "    `f(3, 3) => 6`\n",
    "    \n",
    "    \n",
    "* And so on until we have consumed all values in the RDD:\n",
    "\n",
    "```\n",
    "f(6, 4) => 10\n",
    "f(10, 5) => 15\n",
    "```\n",
    "    \n",
    "In other words: a reduction is essentially walking through a collection and aggregating as we go along. We can do the same thing with a multiplication instead of an addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_tiny = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "rdd_tiny.reduce(lambda a, b: a * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that should make sense now. As an exercise, complete following reduction on our RDD of even numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_even = rdd_even.reduce(FIX_ME) # Complete this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and check the result (`assert` complains when the answer is wrong)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert sum_even == 254500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do something a bit more difficult. We want to calculate the average of the even numbers, using a single reduction. Instead of working with integers, we will work with the [tuple](http://www.tutorialspoint.com/python/python_tuples.htm) data type, representing pairs of (total, count).\n",
    "\n",
    "So the `sum_pairs` function should have the following shape:\n",
    "\n",
    "```\n",
    "f(pair, pair) => pair\n",
    "```\n",
    "    \n",
    "It takes two pairs as input and produce a pair as output. \n",
    "\n",
    "Complete the `sum_pairs` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sum_pairs(pair1, pair2):\n",
    "    # complete this\n",
    "    return FIX_ME\n",
    "\n",
    "total1, count1 = rdd_even.map(lambda x: (x, 1)).reduce(sum_pairs)\n",
    "\n",
    "avg1 = total1 / count1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert avg1 == 509"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous exercise is perhaps the most typical example of a **Map/Reduce** application.\n",
    "\n",
    "We **map** the inputs into a convenient shape, and **reduce** these shapes into the final result.\n",
    "   \n",
    "So to recapitulate:,\n",
    "    \n",
    "* **`map`** performs a function on every single element of a collection, e.g. multiply every item by 2 => the number of elements in the collection does not change,\n",
    "* **`filter`** throws all elements away that do not comply to some rules => the number of elements in the collection is smaller or the same as in the original collection. Different elements are **not** combined.\n",
    "* **`reduce`** combines the different elements of the collection in some way (<-> `filter`)\n",
    "\n",
    "\n",
    "### Aggregating\n",
    "\n",
    "Spark provides several aggregation methods that provide the scaffolding for performing Map/Reduce operations. We now explore on of these methods, called `aggregate`. Let's check out the [documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) again. \n",
    "\n",
    "**`aggregate(zeroValue, seqOp, comboOp)`**\n",
    "\n",
    "Arguably, `aggregate` is the most useful function in the Spark toolbox. It's important to understand how it works.\n",
    "\n",
    "Remember that the purpose of Spark is to perform distributed calculations. \n",
    "\n",
    "In a distributed calculation, the data is first partitioned across different processes on different compute nodes. Next, in each partition, the `zeroValue` and `seqOp` work together to reduce the data to a (partial) aggregation. So in our example that computes the average, we calculate the sum and count per partition with these two functions.\n",
    "\n",
    "* `zeroValue` represents an empty accumulator `acc`.\n",
    "* `seqOp` is a function of shape `(acc, entry) -> acc`, it takes an accumulator (e.g. a `(sum, count)` pair) and produces an updated accumulator.\n",
    "\n",
    "We're not done yet. Next we need to combine the partial results into a final result. That's where the `comboOp` function comes in. \n",
    "\n",
    "* `comboOp` is equivalent to the `f` function we pass to a `reduce(f)`, like illustrated above. It also has the shape `(acc, acc) -> acc`.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "                    zeroValue + seqOp\n",
    "                  +-------------+\n",
    "          ,-----> | partition 1 | ----.\n",
    "         /        +-------------+      \\\n",
    "        /                               \\        comboOp                       \n",
    "+------+          +-------------+        `---> +-------------------------+\n",
    "| Data | -------> | partition 2 | -----------> | [acc 1] [acc 2] [acc 3] | ---> final result\n",
    "+------+          +-------------+        ,---> +-------------------------+\n",
    "        \\                               / \n",
    "         \\        +-------------+      / \n",
    "          `-----> | partition 3 | ----'\n",
    "                  +-------------+\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "The example in the `aggregate` documentation should make sense now. Although the answer is already given, let's do it one more time to train our Spark muscle memory. Show me [wax on, wax off](https://youtu.be/2ynryUjGFt8?t=176)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_value = (0, 0)\n",
    "\n",
    "def seqOp(acc, entry):\n",
    "    # complete this    \n",
    "    return FIX_ME\n",
    "\n",
    "total2, count2 = rdd_even.aggregate(zero_value, seqOp, sum_pairs)\n",
    "\n",
    "avg2 = total2 / count2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting average should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert avg2 == 509"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muy bien.\n",
    "\n",
    "### Aggregating by key\n",
    "\n",
    "A very common principle in data processing is *aggregation by key*. Before we perform an aggregation by key, we need to introduce another Spark concept, the `PairRDD`. A `PairRDD` is an RDD where each item consists of a pair, or tuple of size 2. In such a tuple, the first element is considered the key, the second element the value.\n",
    "\n",
    "A handy function to build a `PairRDD` from a normal RDD is `keyBy(f)`. It takes a function `f` that takes an entry and produces some kind of value for that input entry. For example, key this list of names by their length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names_rdd = sc.parallelize([\"Dirac\", \"Feynman\", \"Ramanujan\"])\n",
    "\n",
    "names_by_length = names_rdd.keyBy(lambda name: len(name))\n",
    "                            \n",
    "names_by_length.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it yourself in this simple example. We use the original rdd and transform it into a new shape, where each integer is keyed by a boolean that says whether it is even or not. We can reuse the `is_even(x)` function we defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_by_key = rdd.keyBy(FIX_ME) # Complete this\n",
    "\n",
    "assert rdd_by_key.take(3) == [(True, 0), (False, 1), (True, 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now perform a reduction by key that returns the sum of even and odd integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_by_key = rdd_by_key.reduceByKey(FIX_ME).collect() # Complete this\n",
    "\n",
    "assert sum_by_key == [(False, 250000), (True, 249500)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last aggregation method we will look at for now is the aggregateByKey method. Again, consult the [documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD).\n",
    "\n",
    "**`aggregateByKey(zeroValue, seqFunc, combFunc)`**\n",
    "\n",
    "Use this to calculate the averages by key instead of the sum by key. We break up the calculation in two steps: first we calculate the sum and count by key, next we calculate the average by key, by using the `mapValues` method. \n",
    "\n",
    "`mapValues` transforms only the \"value\" elements (right) in a `PairRDD`, the keys (left) remain intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_count_by_key = rdd_by_key.aggregateByKey(FIX_ME) # Complete this\n",
    "\n",
    "assert sum_count_by_key.collect() == [(False, (250000, 500)), (True, (249500, 500))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sum_count_to_avg(tuple):\n",
    "    # Complete tis\n",
    "    return FIX_ME\n",
    "\n",
    "avg_by_key = sum_count_by_key.mapValues(sum_count_to_avg).collect() # Complete this\n",
    "\n",
    "assert avg_by_key == [(False, 500.0), (True, 499.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent.\n",
    "\n",
    "We now have some useful methods in our Spark toolbox. We know how to `map`, `filter`, `reduce` and `aggregate`, and we know how to aggregate by key.\n",
    "\n",
    "That made me thirsty, are you thirsty? \n",
    "\n",
    "Let's move on to the beer dataset :-)\n",
    "\n",
    "## Dos cervezas, por favor!\n",
    "\n",
    "We start with reading in the `beers.csv` data set. For convenience, we remove the header from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drop_header(rdd):    \n",
    "    def drop_first(idx, it):\n",
    "        if idx == 0:\n",
    "            next(it)\n",
    "        return it\n",
    "\n",
    "    return rdd.mapPartitionsWithIndex(drop_first)\n",
    "\n",
    "rdd_beers_header = sc.textFile(\"/usr/local/data/beer/beers.csv\")\n",
    "\n",
    "header = rdd_beers_header.first()\n",
    "\n",
    "rdd_beers_unparsed = drop_header(rdd_beers_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which columns does our data set contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Dutch for: \n",
    "\n",
    "`brand, kind, alcohol percentage, brewery`. \n",
    "\n",
    "The first column isn't named but represents record IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_beers_unparsed.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each line in the RDD is just a String for now. This isn't very handy. Let's parse every line into a tuple of size 5: \n",
    "\n",
    "**`(ID, brand, kind, alcohol percentage, brewery)`.**\n",
    "\n",
    "Notice the `parse_pct` function. This function performs necessary *data cleaning* because the data set contains values in the alcohol percentage column that are not floating point numbers ('NA', 'alcoholvrij', etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def only_valid(a):\n",
    "    return a[3] != 'NA'\n",
    "\n",
    "def parse_pct(x):    \n",
    "    if 'alcoholvrij' in x: # alcohol free\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(x)    \n",
    "    \n",
    "def parse_beer(a):    \n",
    "    try:\n",
    "        return (int(a[0]), a[1], a[2], parse_pct(a[3]), a[4])\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "\n",
    "rdd_beers = rdd_beers_unparsed.map(lambda s: s.split(\",\")).filter(only_valid).map(FIX_ME) # Complete this\n",
    "\n",
    "rdd_beers.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start with the exercises, we define a few accessor functions for our beer tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ID(beer_tuple):\n",
    "    return beer_tuple[0]\n",
    "\n",
    "def brand(beer_tuple):\n",
    "    return beer_tuple[1]\n",
    "\n",
    "def kind(beer_tuple):\n",
    "    return beer_tuple[2]\n",
    "\n",
    "def alcohol_pct(beer_tuple):\n",
    "    return beer_tuple[3]\n",
    "\n",
    "def brewery(beer_tuple):\n",
    "    return beer_tuple[4]\n",
    "\n",
    "def key(pair):\n",
    "    return pair[0]\n",
    "\n",
    "def value(pair):\n",
    "    return pair[1]\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to demonstrate your [Spark-fu](https://youtu.be/6vMO3XmNXe4?t=7) skills on the beer data set!\n",
    "\n",
    "Good luck.\n",
    "\n",
    "### Exercise 1: number of breweries (easy)\n",
    "\n",
    "How many different breweries do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distinct_breweries = rdd_beers.map(FIX_ME).FIX_ME # Complete this\n",
    "\n",
    "nr_breweries = distinct_breweries.FIX_ME # Complete this\n",
    "\n",
    "assert nr_breweries == 361"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: strongest beer (medium)\n",
    "\n",
    "Which beer is the strongest one? (highest alcohol percentage). Use `reduce` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def max_pct(beer1, beer2):\n",
    "    return FIX_ME # Complete this\n",
    "\n",
    "beer_max_pct = rdd_beers.reduce(max_pct) \n",
    "\n",
    "# Verify you solution\n",
    "assert beer_max_pct[3] == 26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is the `max(f)` method, where you pass a selector function `f` that is used for comparing items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beer_max_pct_2 = rdd_beers.max(FIX_ME) # Complete this\n",
    "\n",
    "# Verify your solution\n",
    "assert beer_max_pct_2[3] == 26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: most common alcohol percentages (medium)\n",
    "\n",
    "How many beers are there for each alcohol percentage? Which 3 alcohol percentages are the most common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def beers_per_pct():\n",
    "    \n",
    "    zero = FIX_ME # Complete this\n",
    "    \n",
    "    def seqOp(acc, entry):\n",
    "        return FIX_ME # Complete this\n",
    "    \n",
    "    def comboOp(acc1, acc2):\n",
    "        return FIX_ME # Complete this\n",
    "    \n",
    "    def selector(tuple):\n",
    "        return FIX_ME # Complete this\n",
    "    \n",
    "    return rdd_beers.keyBy(FIX_ME).aggregateByKey(zero, seqOp, comboOp).sortBy(selector, ascending = False)\n",
    "\n",
    "# Sanity check\n",
    "assert rdd_beers.count() == beers_per_pct().map(value).sum()\n",
    "\n",
    "# Verify you solution\n",
    "assert beers_per_pct().take(3) == [(8.0, 180), (6.5, 150), (5.0, 131)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: brewery with strongest average beer (advanced)\n",
    "\n",
    "Find the brewery that has the highest average alcohol percentage of all beers it makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strongest_average():\n",
    "    \n",
    "    zero = FIX_ME # Complete this\n",
    "    \n",
    "    def seqOp(acc, beer): \n",
    "        return FIX_ME # Complete this\n",
    "    \n",
    "    def comboOp(acc1, acc2): \n",
    "        return FIX_ME # Complete this\n",
    "    \n",
    "    def avg(tuple):\n",
    "        return FIX_ME # Complete this\n",
    "    \n",
    "    return rdd_beers.keyBy(FIX_ME).aggregateByKey(zero, seqOp, comboOp).mapValues(avg).max(value)\n",
    "\n",
    "assert strongest_average() == ('Staminee De Garre (Brouwerij Van Steenberge)', 11.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: breweries with most kinds of beers (advanced)\n",
    "\n",
    "Return the top 3 breweries with most kinds of beers, also return these kinds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def diverse_breweries():\n",
    "    \n",
    "    zero = FIX_ME # Complete this\n",
    "    \n",
    "    def seqOp(acc, entry):\n",
    "        return FIX_ME # Complete this\n",
    "    \n",
    "    def comboOp(acc1, acc2):\n",
    "        return FIX_ME # Complete this\n",
    "    \n",
    "    def selector(tuple):\n",
    "        return FIX_ME # Complete this\n",
    "    \n",
    "    return rdd_beers.keyBy(FIX_ME).aggregateByKey(zero, seqOp, comboOp).sortBy(selector, ascending = False)\n",
    "    \n",
    "# Verify your answer\n",
    "assert diverse_breweries().map(key).take(3) == ['Brouwerij Huyghe', 'Brouwerij Alvinne', 'Brouwerij Bavik']\n",
    "\n",
    "# Show us the result\n",
    "diverse_breweries().take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Voila!\n",
    "\n",
    "This completes your [Spark-fu](https://youtu.be/84VtdVK2a0A?t=219) training, oh mighty dragon warrior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
